{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import missingno as msno\n",
    "from survey_data import SurveyData\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "import scikit_posthocs as sp\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy import stats\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# from textblob import TextBlob\n",
    "# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from wordcloud import WordCloud\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  build folder for each part (automated)\n",
    "\n",
    "folders = ['Graphs/Qs', 'Graphs/Rating', 'Graphs/Missing data', 'Graphs/Feedbacks', 'Graphs/Satisfaction', 'Graphs/Feedbacks']\n",
    "\n",
    "# Create the directory\n",
    "for directory in folders:\n",
    "    os.makedirs(directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_df_list = []  # each element of the list contains the data of a department\n",
    "source_list = [\"Business School\", \"CM\", \"CQT\", \"ETP\", \"ISS\", \"IVO\", \"LKYSPP\", \"Medical School\", \"ODPRT\", \"SCALE\", \"UCI\"]\n",
    "for source in source_list:\n",
    "    location = f'Raw Results/NUS {source}.xlsx'\n",
    "    df = SurveyData(source, location)\n",
    "    df_raw = df.load_data()\n",
    "    df_dropped = df.data_drop(df_raw)\n",
    "    # df_dropped = df_dropped.drop('Progress', axis=1)\n",
    "    df_coded = df.coding(df_dropped)\n",
    "    # print(df_coded)\n",
    "    df_no_rank = df.del_rank(df_coded)\n",
    "    # df_filled = df_no_rank.fillna(0)\n",
    "    # df_filled.head()\n",
    "    dep_df_list.append(df_coded)\n",
    "\n",
    "df_overall = pd.concat(dep_df_list, axis=0, ignore_index = True)\n",
    "print(df_overall.head())\n",
    "print(f'overall df info:\\n{df_overall.info()}')\n",
    "print(f'count:\\n{df_overall.count()}')\n",
    "df_overall_Q = df_overall.iloc[:,3:-1]\n",
    "# print(\"describe overall data:\")\n",
    "# print(df_overall.describe())\n",
    "# merge_data.visualize_Qs(df_overall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_missing_data_perc(df, data_source):\n",
    "    plt.figure(figsize=(12,6))\n",
    "    sns.barplot(df.isna().sum()/len(df)*100)\n",
    "    plt.axhline(y=5, color='red', linestyle='--')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(f'Percentage of {data_source} missing data')\n",
    "    plt.ylabel('percentage of missing data (%)')\n",
    "    plt.savefig(f'Graphs/Missing data/Percentage of {data_source} missing data.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Missing data for each Q\n",
    "If missing data is more than 5%, instead of deleting them, other options should be taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall num\n",
    "print(f'overall missing data:\\n {df_overall.isna().sum()}\\n')\n",
    "\n",
    "# visualiza overall missing data for each Q\n",
    "draw_missing_data_perc(df_overall, 'overall')\n",
    "# tips: savefig before show\n",
    "\n",
    "\n",
    "# deal with the missing values\n",
    "# percentage of missing rows\n",
    "max_missing_Q = df_overall_Q.isna().any(axis=1).sum()  # if there is at least one True (i.e., at least one NaN) across all columns\n",
    "percentage_missing = max_missing_Q/len(df_overall) * 100\n",
    "print(f'max percentage of missing data (rows): {percentage_missing:.2f}%, overall num is {len(df_overall)}, the max missing num of Question is {max_missing_Q}')\n",
    "df_overall_cleaned = df_overall_Q[~df_overall_Q.isna().any(axis=1).values]\n",
    "df_overall_cleaned = df_overall_cleaned.reset_index(drop=True)\n",
    "overall_num = len(df_overall_cleaned)\n",
    "print(f'length of cleaned overall data: {overall_num}')\n",
    "print(f'head of overall cleaned data:\\n{df_overall_cleaned.head()}')\n",
    "\n",
    "dep_df_cleaned_list = []\n",
    "for i in range(len(source_list)):\n",
    "    dep_df_Q = dep_df_list[i].iloc[:,3:-1]\n",
    "    dep_df_cleaned = dep_df_Q[~dep_df_Q.isna().any(axis=1).values]\n",
    "    dep_df_cleaned = dep_df_cleaned.reset_index(drop=True)\n",
    "    dep_df_cleaned_list.append(dep_df_cleaned)\n",
    "print(f'cleaned data for first department: \\n{dep_df_cleaned_list[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing data for each department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each department num\n",
    "for i,dep in enumerate(source_list):\n",
    "    department = source_list[i]\n",
    "    dep_df = dep_df_list[i]\n",
    "    # print(f'{department} missing data:\\n{dep_df.isna().sum()}\\n')\n",
    "    draw_missing_data_perc(dep_df, dep + ' department')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Descriptive statistics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Count, percentage (Overall Satisfaction,SP,SN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_distri_each_Q(y, y_label,path,average=False):\n",
    "    plt.clf()\n",
    "    x = ['Q'+str(i) for i in range(1,16)]\n",
    "    colors = ['blue'] * len(x)\n",
    "    min_idx = y.index(min(y))\n",
    "    max_idx = y.index(max(y))\n",
    "    # Change the color of the bar at index 2 (bar for 'C') to red\n",
    "    colors[min_idx] = 'red'\n",
    "    colors[max_idx] = 'green'\n",
    "    plt.bar(x,y, color = colors, alpha=0.6)\n",
    "    if average == True:\n",
    "        m = float(np.mean(y))\n",
    "        plt.axhline(m, color='grey', linestyle='--', linewidth=2, label='Average')\n",
    "        plt.text(x=x[0],y=m,s=f'{round(m,2)}',color='grey')\n",
    "        plt.legend()\n",
    "    for i in range(len(x)):\n",
    "        plt.text(x=x[i], \n",
    "                y=y[i], \n",
    "                s=f\"{y[i]}\", \n",
    "                ha='center', \n",
    "                va='bottom')\n",
    "    plt.xticks(rotation=45, ha='right') # Rotate x labels for better readability\n",
    "    y_upper = max(y)+0.01\n",
    "    y_lower = max(min(y)-0.01,0)\n",
    "    # print(f'y range: {y_lower} to {y_upper}')\n",
    "    plt.ylim(y_lower,y_upper)\n",
    "    plt.xlabel('Questions')\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(f'{y_label} in each Question')\n",
    "    plt.show()\n",
    "    plt.savefig(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_cols = ['Q'+str(i) for i in range(1,16)]\n",
    "combined_cols = Q_cols + ['Department']\n",
    "combined_dfs = pd.DataFrame(columns=combined_cols)  # combine all data into one df\n",
    "\n",
    "\n",
    "for idx,dep in enumerate(source_list):\n",
    "    df_dep = dep_df_cleaned_list[idx]\n",
    "    df_dep['Department'] = dep\n",
    "    # Concatenate along rows (axis=0)\n",
    "    combined_dfs = pd.concat([combined_dfs, df_dep], axis=0, ignore_index=True)\n",
    "print(f'head of combined_dfs:\\n{combined_dfs.head()}')\n",
    "\n",
    "# answer of each Q (performance, how satisfied about it)\n",
    "Q_Score_count = [combined_dfs[x].value_counts().reset_index() for x in Q_cols]\n",
    "for c in Q_Score_count:\n",
    "    c['percentage'] = round(c['count']/overall_num,2)\n",
    "    c.columns = ['score','count','percentage']\n",
    "print(Q_Score_count)\n",
    "\n",
    "# for each question (satisfaction)\n",
    "Q_performance = [round(df.sort_values(by='score', ascending=False).loc[0,'percentage'] + df.sort_values(by='score', ascending=False).loc[1, 'percentage'],2) for df in Q_Score_count]\n",
    "y_label = 'Overall Satisfaction (Strongly positive + positive)'\n",
    "visualize_distri_each_Q(Q_performance, y_label, f'Graphs/Satisfaction/{y_label}.png',average=True)\n",
    "\n",
    "Q_SP = [df.sort_values(by=['score'], ascending=False).reset_index().loc[0,'percentage'] for df in Q_Score_count]\n",
    "print('Q_SP:', Q_SP) \n",
    "y_label = 'Q_SP(strongly positive)'\n",
    "visualize_distri_each_Q(Q_SP, y_label, f'Graphs/Satisfaction/{y_label}.png')\n",
    "\n",
    "Q_SN = [df.sort_values(by='score', ascending=True).reset_index().loc[0,'percentage'] for df in Q_Score_count]\n",
    "print('Q_SN:', Q_SN) \n",
    "y_label = 'Q_SN(strongly negative)'\n",
    "visualize_distri_each_Q(Q_SN, y_label, f'Graphs/Satisfaction/{y_label}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each Q\n",
    "Q_mean_df = combined_dfs[Q_cols].mean().reset_index()\n",
    "Q_mean_df.columns = ['Q', 'mean']\n",
    "Q_mean_df['mean']= Q_mean_df['mean'].astype(float)\n",
    "Q_mean_df['mean'] = round(Q_mean_df['mean'],2)\n",
    "\n",
    "# Q_mean_df\n",
    "\n",
    "visualize_distri_each_Q(Q_mean_df['mean'].tolist(), 'Mean value', 'Graphs/Qs/Mean for Each Question.png',average=True)\n",
    "\n",
    "## conclusion: Q6 is the lowest, while Q2 hightest\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Variation\n",
    "Not very useful in this case: all answers are categorical variables with 5 possible values, so it doesn't make much sense to see how they vary. But it can still show the distribution to some degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_std_df = combined_dfs[Q_cols].std().reset_index()\n",
    "Q_std_df.columns = ['Q', 'std']\n",
    "Q_std_df['std']= Q_std_df['std'].astype(float)\n",
    "Q_std_df['std'] = round(Q_std_df['std'],2)\n",
    "\n",
    "# Q_std_df\n",
    "\n",
    "visualize_distri_each_Q(Q_std_df['std'].tolist(), 'Standard deviation', 'Graphs/Qs/Std for Each Question.png')\n",
    "\n",
    "\n",
    "## conclusion: pretty close, not a good indicator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 CV (Coefficient of variation)\n",
    "As each question has same categorical variable answer, variance related metrics cannot tell us much about the distribtion (central tendency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV for each question (scale): also help to determine the weights\n",
    "combined_df = pd.merge(Q_std_df, Q_mean_df, on='Q')\n",
    "Q_CV_df = combined_df.copy()\n",
    "Q_CV_df['CV'] = round((Q_CV_df['std'] / Q_CV_df['mean']) ,2)\n",
    "# Q_CV_df = Q_CV_df.sort_values(by=['CV','mean'], ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(f'Q_CV_df_sorted:\\n{Q_CV_df}')\n",
    "# CV_df.to_excel('Qs.xlsx', index=False)\n",
    "\n",
    "visualize_distri_each_Q(Q_CV_df['CV'].tolist(), 'CV', 'Graphs/Qs/CV for Each Question.png')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Percentage of neutral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of neutral for each Q\n",
    "def neutral_percentage(combined_dfs,Q_cols):\n",
    "    neu_perc_list = []\n",
    "    for q in Q_cols:\n",
    "        neutral_num = combined_dfs[[q]].value_counts()[0]\n",
    "        overall_num = combined_dfs[q].count()\n",
    "        neutral_perc = round(neutral_num/overall_num,2)\n",
    "        neu_perc_list.append(neutral_perc)\n",
    "    print(f'percentage of neutral answers over the overall answers:{neu_perc_list}')\n",
    "    visualize_distri_each_Q(neu_perc_list, 'Neutral percentage','Graphs/Qs/Neutral percentage of each Q',average=True)\n",
    "    high_perc = [i for i, perc in enumerate(neu_perc_list) if perc > 0.2]\n",
    "    high_perc_Qs = [Q_cols[i] for i in high_perc]\n",
    "    # for Qs that have high neutral percentage, see the department\n",
    "    neu_dep_df = pd.DataFrame(columns=['Department'] + high_perc_Qs)\n",
    "    \n",
    "    for h in high_perc_Qs:\n",
    "        df = combined_dfs[[h,'Department']]\n",
    "        # percentage of neutral in the department\n",
    "        count_df = df.groupby('Department')[h].apply(lambda x: (x == 0).sum()).reset_index(name='ZeroCount')\n",
    "        popu_df = df.groupby('Department').size().reset_index(name='Population')\n",
    "        count_df = pd.merge(count_df, popu_df, on='Department')\n",
    "        count_df[h] = count_df['ZeroCount'] / count_df['Population']\n",
    "        count_df[h] = count_df[h].round(2)\n",
    "        count_df = count_df.drop(columns=['ZeroCount', 'Population'])\n",
    "\n",
    "        if neu_dep_df.empty:\n",
    "            neu_dep_df = count_df\n",
    "        else:\n",
    "            # Merge on 'Department' for subsequent iterations\n",
    "            neu_dep_df = pd.merge(neu_dep_df, count_df[['Department',h]], on='Department')\n",
    "    \n",
    "    neu_dep_df.loc[len(neu_dep_df)] = ['All'] + neu_dep_df[high_perc_Qs].mean().tolist()\n",
    "    print(neu_dep_df)\n",
    "    last_row = neu_dep_df.iloc[-1]\n",
    "    last_row = pd.to_numeric(last_row, errors='coerce')\n",
    "    top_three = last_row.nlargest(3)\n",
    "    print(f'Questions with largest percentage of neutral answers (denominator as number of responses of the department):')\n",
    "    for metric, value in top_three.items():\n",
    "        print(f'{metric}: {value:.2f}')\n",
    "    neu_dep_df = neu_dep_df.set_index('Department')\n",
    "\n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(neu_dep_df, annot=True, cmap=sns.cubehelix_palette(as_cmap=True), cbar=True, fmt=\".2f\")\n",
    "\n",
    "    # Add titles and labels\n",
    "    plt.title('Heatmap of Proportions of Neutral Answer by Department')\n",
    "    plt.xlabel('Questions')\n",
    "\n",
    "neutral_percentage(combined_dfs,Q_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranking for overall satisfaction, percentage, mean, CV -> visualization\n",
    "def rank_Qs(df, y):\n",
    "    # from max to min\n",
    "    sorted_df = df.sort_values(by = y, ascending=False).reset_index()\n",
    "    values = sorted_df[y].tolist()\n",
    "    Q_rank = sorted_df['Q'].tolist()\n",
    "    # print(values)\n",
    "    # print(Q_rank)\n",
    "    print(f'maximum {y} value {values[0]} of is for {sorted_df.loc[0,'Q']}, minimum {y} value {values[-1]} is for {sorted_df.loc[len(values)-1,'Q']}')\n",
    "    return Q_rank\n",
    "CV_rank = rank_Qs(Q_CV_df,'CV')\n",
    "mean_rank = rank_Qs(Q_mean_df,'mean')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Extraction for Overall Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Score of overall rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be checked\n",
    "list_0 = [\"CM\", \"ISS\", \"LKYSPP\", \"Medical School\", \"UCI\"]\n",
    "list_1 = [\"Business School\", \"CQT\", \"ETP\", \"IVO\", \"ODPRT\", \"SCALE\"]\n",
    "\n",
    "rating_list = [pd.read_excel(f'Raw Results/NUS {i}.xlsx').iloc[:, -1].dropna()\n",
    "    for i in list_0] + [pd.read_excel(f'Raw Results/NUS {j}.xlsx').iloc[:, -2].dropna()\n",
    "    for j in list_1]\n",
    "# print(rating_list)\n",
    "\n",
    "rating_list_new = []\n",
    "# add deparment (to use groupby method)\n",
    "for i in range(len(rating_list)):\n",
    "    df = pd.DataFrame(rating_list[i])\n",
    "    ordered_list = list_0 + list_1\n",
    "    df['Department'] = ordered_list[i]\n",
    "    df.columns = ['Overall Rating', 'Department']\n",
    "    rating_list_new.append(df)\n",
    "# print(rating_list_new[0])\n",
    "aggre_rating_df = pd.concat(rating_list_new, ignore_index=True)\n",
    "print('aggre_rating_df:', aggre_rating_df)\n",
    "\n",
    "\n",
    "## Box plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Department', y='Overall Rating', data=aggre_rating_df)\n",
    "plt.title('Box Plot of Ratings by Department')\n",
    "plt.xlabel('Department')\n",
    "plt.ylabel('Overall Rating')\n",
    "plt.xticks(rotation=45)  # Rotate x labels if needed\n",
    "# plt.show()\n",
    "plt.savefig('Graphs/Rating/box_plot.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_distri_each_dep(y,y_label,path):\n",
    "    plt.clf()\n",
    "    x = [\"Business School\", \"CM\", \"CQT\", \"ETP\", \"ISS\", \"IVO\", \"LKYSPP\", \"Medical School\", \"ODPRT\", \"SCALE\", \"UCI\"]\n",
    "    colors = ['blue'] * len(x)\n",
    "    min_idx = y.index(min(y))\n",
    "    max_idx = y.index(max(y))\n",
    "    # Change the color of the bar at index 2 (bar for 'C') to red\n",
    "    colors[min_idx] = 'red'\n",
    "    colors[max_idx] = 'green'\n",
    "    plt.bar(x,y, color = colors, alpha=0.6)\n",
    "    mean_val = np.mean(y)\n",
    "    horizontal_line_value = mean_val  # The y-coordinate where the horizontal line will be drawn\n",
    "    plt.text(x=x[0], y=mean_val, s = round(mean_val,2),  color = 'grey')\n",
    "    plt.axhline(y=horizontal_line_value, color='grey', linestyle='--', linewidth=2, label='Average score for all departments')\n",
    "    for i in range(len(x)):\n",
    "        plt.text(x=x[i], \n",
    "                y=y[i], \n",
    "                s=f\"{y[i]}\", \n",
    "                ha='center', \n",
    "                va='bottom')\n",
    "    plt.xticks(rotation=45, ha='right') # Rotate x labels for better readability\n",
    "    y_upper = max(y)+0.01\n",
    "    y_lower = min(y)-0.01\n",
    "    # print(f'y range: {y_lower} to {y_upper}')\n",
    "    plt.ylim(y_lower,y_upper)\n",
    "    plt.xlabel('Department')\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(f'{y_label} for each department')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.savefig(path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each department \n",
    "## Mean of each department\n",
    "mean_rating_df = aggre_rating_df.groupby('Department')['Overall Rating'].mean().reset_index()\n",
    "# print('mean_rating_df:', mean_rating_df)\n",
    "mean_rating_df['Overall Rating'] = mean_rating_df['Overall Rating'].astype('float')\n",
    "mean_rating_df['Overall Rating'] = round(mean_rating_df['Overall Rating'],2)\n",
    "overall_rating_score = mean_rating_df['Overall Rating']\n",
    "# print('overall_rating:', overall_rating)\n",
    "visualize_distri_each_dep(mean_rating_df['Overall Rating'].tolist(), 'Mean of Overall Rating', 'Graphs/Rating/mean_histogram.png')\n",
    "\n",
    "\n",
    "## variance\n",
    "var_rating_df = aggre_rating_df.groupby('Department')['Overall Rating'].var().reset_index()\n",
    "# print(\"var_rating_df:\", var_rating_df)\n",
    "var_rating_df['Overall Rating'] = var_rating_df['Overall Rating'].astype('float')\n",
    "var_rating_df['Overall Rating'] = round(var_rating_df['Overall Rating'],2)\n",
    "visualize_distri_each_dep(var_rating_df['Overall Rating'].tolist(), 'Variance of Overall Rating', 'Graphs/Rating/var_histogram.png')\n",
    "\n",
    "\n",
    "## Coefficient of Variation (CV) and Index Score\n",
    "std_rating_df = aggre_rating_df.groupby('Department')['Overall Rating'].std().reset_index()\n",
    "std_rating_df.columns = ['Department', 'Standard Deviation']\n",
    "mean_rating_df.columns = ['Department', 'Mean']\n",
    "# print(\"std_rating_df:\", std_rating_df)\n",
    "combined_df = pd.merge(std_rating_df, mean_rating_df, on='Department')\n",
    "combined_df_CV = combined_df.copy()\n",
    "combined_df_CV['CV (%)'] = (combined_df_CV['Standard Deviation'] / combined_df_CV['Mean']) * 100\n",
    "\n",
    "\n",
    "combined_df_score = combined_df.copy()\n",
    "combined_df_score['Score'] = combined_df_CV['Mean'] - 0.5 * combined_df_CV['Standard Deviation'] \n",
    "combined_df_score['Score'] = combined_df_score['Score'].astype('float')\n",
    "combined_df_score['Score'] = round(combined_df_score['Score'],2)\n",
    "#  [6.99, 6.74, 6.16, 6.62, 6.79, 7.66, 6.7, 6.88, 6.76, 6.92, 6.95]\n",
    "visualize_distri_each_dep(combined_df_score['Score'].tolist(), 'Index Score (mean - 0.5*std)', 'Graphs/Rating/index_score_histogram(CV,mean).png')\n",
    "\n",
    "\n",
    "## conclusion from Score Index: IVO best; Business second best; CQT worst; else average.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Score of 15 questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just use the mean of all Questions\n",
    "Qs_mean = Q_mean_df['mean'].mean()\n",
    "Qs_score = round((Qs_mean+2)/4 * 100,2)\n",
    "print(f'Q_score:{Qs_score}')\n",
    "\n",
    "# for each department\n",
    "dep_mean_df = combined_dfs.groupby('Department')[Q_cols].mean().reset_index()\n",
    "dep_mean_df[Q_cols] = dep_mean_df[Q_cols].astype('float')\n",
    "dep_mean_df['Qs mean'] = dep_mean_df[Q_cols].mean(axis=1)\n",
    "dep_mean_df['Qs score'] = (dep_mean_df['Qs mean'] + 2)/4\n",
    "Qs_score =  round(dep_mean_df['Qs score'],2)\n",
    "\n",
    "visualize_distri_each_dep(Qs_score.tolist(), 'Score for 15 questions', 'Graphs/Qs/Qs score for each department.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Final Score of performance of each department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(mean_rating_df)\n",
    "mean_Qs_df = dep_mean_df[['Department', 'Qs mean']]\n",
    "overall_score_df = pd.merge(mean_rating_df,mean_Qs_df, on='Department')\n",
    "overall_score_df.columns = ['department', 'rating mean', 'Qs mean']\n",
    "overall_score_df['Qs mean'] = (overall_score_df['Qs mean']+2)/4 * 100\n",
    "overall_score_df['overall score'] = overall_score_df[\"rating mean\"]*10*0.4 + overall_score_df['Qs mean']*0.6\n",
    "overall_score_df['overall score'] = round(overall_score_df['overall score'],2)\n",
    "print(overall_score_df)\n",
    "overall_score_mean = overall_score_df['overall score'].mean()\n",
    "print(f'the final score for the whole school: {overall_score_mean.round(2)}')\n",
    "visualize_distri_each_dep(overall_score_df['overall score'].tolist(), 'Final score', 'Graphs/Fianl score of each department')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Comparison among departments (hypothesis test, for each Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New (More suitable one) : Kruskal-Wallis Test (for non-normal distribution, rank sum) + Dunn's test (post-hoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each Q, check whether there is significant different among departments\n",
    "signis_qs= []\n",
    "alpha = 0.05\n",
    "diff_deps = {key:[] for key in Q_cols}\n",
    "for q in Q_cols:\n",
    "    data_list = [combined_dfs[combined_dfs['Department']==d][q] for d in source_list]\n",
    "    combined_dfs['department_idx'] = combined_dfs['Department'].replace({key:source_list.index(key) for key in source_list})\n",
    "    result = stats.kruskal(*data_list)\n",
    "    p = result.pvalue\n",
    "    # print(f'p value:{p}')\n",
    "    # p_values.append(p)\n",
    "    if p < alpha:\n",
    "        print(f'\\n{q} has significant differences. Performing Dunn\\'s test...')\n",
    "        \n",
    "\n",
    "        # Combine data for Dunn's test\n",
    "        df = combined_dfs[[q,'department_idx']]\n",
    "\n",
    "        # Perform Dunn's test\n",
    "        dunn_results = sp.posthoc_dunn(combined_dfs, val_col=q, group_col='department_idx', p_adjust='bonferroni')\n",
    "        # print(dunn_results)\n",
    "\n",
    "        if (dunn_results < alpha).any().any():\n",
    "            signis_qs.append(q)\n",
    "\n",
    "            # Initialize a dictionary to count wins for each department\n",
    "            win_count = {dep: 0 for dep in source_list}\n",
    "            lose_count = {dep: 0 for dep in source_list}\n",
    "\n",
    "            # Loop through the Dunn's test results to determine wins\n",
    "            for i, dep1 in enumerate(dunn_results.index):\n",
    "                for j, dep2 in enumerate(dunn_results.columns):\n",
    "                    if i != j:  # Ignore diagonal\n",
    "                        if dunn_results.loc[dep1, dep2] < alpha:  # Significant difference\n",
    "                            # Compare medians to decide the winner\n",
    "                            mean1 = combined_dfs[combined_dfs['Department'] == source_list[i]][q].mean()\n",
    "                            mean2 = combined_dfs[combined_dfs['Department'] == source_list[j]][q].mean()\n",
    "                            \n",
    "                            if mean1 > mean2:  # Assume higher median is better\n",
    "                                win_count[source_list[dep1]] += 1\n",
    "                                lose_count[source_list[dep2]] += 1\n",
    "                            else:\n",
    "                                win_count[source_list[dep2]] += 1\n",
    "                                lose_count[source_list[dep1]] += 1\n",
    "\n",
    "            # Calculate win rate for each department\n",
    "            total_comparisons = len(source_list) - 1  # Each department is compared to others\n",
    "            win_rate = {dep: wins / total_comparisons for dep, wins in win_count.items()}\n",
    "            lose_rate = {dep: loses / total_comparisons for dep, loses in lose_count.items()}\n",
    "\n",
    "            # Print win rates\n",
    "            print(\"Win rates for each department based on Dunn's test results:\")\n",
    "            for department, rate in win_rate.items():\n",
    "                if rate > 0:\n",
    "                    print(f\"{department}: {rate:.2f}\")\n",
    "            print(\"Lose rates for each department based on Dunn's test results:\")\n",
    "            for department, rate in lose_rate.items():\n",
    "                if rate > 0:\n",
    "                    print(f\"{department}: {rate:.2f}\")\n",
    "            \n",
    "            num_wins = np.unique(list(win_rate.values()))\n",
    "            if len(num_wins)>1:\n",
    "                diff_deps[q].append(max(win_rate, key=lambda k: win_rate[k]))\n",
    "            num_loses = np.unique(list(lose_rate.values()))\n",
    "            if len(num_loses)>1:\n",
    "                diff_deps[q].append(max(lose_rate, key=lambda k: lose_rate[k]))\n",
    "        else:\n",
    "            print(f'{q}: No significant pairs for Dunns test!')\n",
    "    else:\n",
    "        print(f\"\\n{q} Kruskal-Wallis test is not significant (p = {p}). No further pairwise comparisons needed.\")\n",
    "\n",
    "# Determine which questions have p-values less than alpha\n",
    "# reject_list = p_values_series[p_values_series < alpha].index.tolist()\n",
    "# print(f'Qs that are significantly different across departments:{reject_list}')\n",
    "print(signis_qs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization: mean, show which deparments are significantly different\n",
    "plt.figure(figsize=(15, 10))\n",
    "ncols = len(signis_qs) // 3 + 1\n",
    "for i,q in enumerate(signis_qs):\n",
    "    y = dep_mean_df[q]\n",
    "    x = source_list\n",
    "    plt.subplot(3, ncols, i + 1)\n",
    "    # plt.xlabel('Departments')\n",
    "    bars = sns.barplot(x=x,y=y)\n",
    "    plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels by 45 degrees\n",
    "    plt.title(f'Mean of {q} cross departments')\n",
    "    for j in range(len(y)):\n",
    "        plt.text(x=x[j], y=y[j],s=f\"{y[j]:.2f}\", ha='center')\n",
    "    diff_dep = diff_deps[q]\n",
    "    diff_dep = [source_list.index(x) for x in diff_dep]\n",
    "    # print(diff_dep)\n",
    "    for dep in diff_dep:\n",
    "        # Access the bar patch using the index and change its color\n",
    "        if dep in [y.argmax(), y.argmin()]:\n",
    "            bars.patches[dep].set_facecolor('red')  # Change the color of the specific bar\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig('Graphs/Qs/Mean with significant difference cross departments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Radar chart for each department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_meaning_dict = {\n",
    "    'Q1': \"Ethical Standards of Management\",\n",
    "    'Q2': \"Ethical Standards of Supervisor\",\n",
    "    'Q3': \"Sensitivity to Ethical Considerations\",\n",
    "    'Q4': \"Trust and Communication\",\n",
    "    'Q5': \"Fair Treatment\",\n",
    "    'Q6': \"Pressure to Compromise Ethics\",\n",
    "    'Q7': \"Internal Controls Effectiveness\",\n",
    "    'Q8': \"Disciplinary Actions for Violations\",\n",
    "    'Q9': \"Whistleblowing Reporting Knowledge\",\n",
    "    'Q10': \"Whistleblowing Channel Confidentiality\",\n",
    "    'Q11': \"Protection for Whistleblowers\",\n",
    "    'Q12': \"Raising Ethical Concerns\",\n",
    "    'Q13': \"Resolution of Internal Complaints\",\n",
    "    'Q14': \"Resolution of Public Feedback\",\n",
    "    'Q15': \"Ethics Training Programs\"\n",
    "} \n",
    "\n",
    "def radar_chart(dep, Q_list, df): # use mean value\n",
    "    if dep != 'All':\n",
    "        values = df[df['Department']==dep][Q_list].mean().tolist()\n",
    "    else:\n",
    "        values = df[Q_list].mean().tolist()\n",
    "    # print(values)\n",
    "    theta = [Q_meaning_dict[q] for q in Q_list]\n",
    "    rc_df = pd.DataFrame(dict(r=values,\n",
    "                              theta = theta))\n",
    "    fig = px.line_polar(rc_df, r='r', theta='theta', line_close=True)\n",
    "    fig.update_traces(fill='toself')\n",
    "    fig.update_layout(\n",
    "    title=f'Performance in {dep} Department',\n",
    "    title_font_size=20,\n",
    "    margin=dict(l=60, r=60, t=60, b=60),  # Increase margins to ensure labels fit\n",
    "    polar=dict(\n",
    "        angularaxis=dict(\n",
    "            tickfont_size=12,  # Adjust font size for theta labels\n",
    "            rotation=90,  # Rotate the chart to better fit labels\n",
    "            direction=\"clockwise\",  # Optional: make it clockwise\n",
    "        )\n",
    "    ),\n",
    "    width=700,  # Adjust width for compactness\n",
    "    height=400  # Adjust height for compactness\n",
    ")\n",
    "    fig.show()\n",
    "    # fig.write_image(\"plotly_chart.png\")\n",
    "\n",
    "# combined_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radar_chart('Business School', ['Q1','Q5','Q6','Q13','Q14'], combined_dfs)\n",
    "radar_chart('IVO', ['Q1','Q5','Q6','Q13','Q14'], combined_dfs)\n",
    "radar_chart('CQT', ['Q1','Q5','Q6','Q13','Q14'], combined_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radar_chart('All', ['Q1','Q5','Q6','Q13','Q14'], combined_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordcloud (for feedbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_list = [pd.read_excel(f'Raw Results/NUS {source}.xlsx').iloc[:, -1].dropna() for source in source_list]\n",
    "\n",
    "for i in range(len(feedback_list)):\n",
    "    fb = pd.DataFrame(feedback_list[i])\n",
    "    fb = fb.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "    # Remove all rows containing the value 'nil'\n",
    "    fb = fb[~fb.apply(lambda row: row.astype(str).str.contains('nil').any(), axis=1)]\n",
    "    # print('fb:', fb)\n",
    "    feedback_list[i] = fb\n",
    "# print('feedback_list:', feedback_list)\n",
    "\n",
    "\n",
    "# Function to filter out feedbacks with fewer than 3 words\n",
    "def filter_feedback(feedback):\n",
    "    return ' '.join(feedback for feedback in feedback if len(feedback.split()) >= 3)\n",
    "combined_feedback = ' '.join(\n",
    "    filter_feedback(feedback_series)\n",
    "    for feedback_series in feedback_list)\n",
    "\n",
    "\n",
    "##  Wordcloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(combined_feedback)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  # Hide the axis\n",
    "# plt.show()\n",
    "wordcloud.to_file('Graphs/Feedbacks/wordcloud.png')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
